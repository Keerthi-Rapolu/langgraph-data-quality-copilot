# ğŸ™ LangGraph Data Quality Copilot (Local LLM + DuckDB)

Imagine this.

You just got a CSV from â€œsomewhereâ€ (email, S3 export, vendor drop, quick extract).
You *know* itâ€™s going to have duplicates, missing values, weird rangesâ€¦  
But you donâ€™t want to spend your evening writing 20 validation queries.

So I built a copilot.

This project is a **local-first Data Quality agent** that:
1) loads a dataset into DuckDB  
2) profiles the data like a curious analyst  
3) asks a local LLM (Ollama) to propose quality rules  
4) validates those rules against the dataset  
5) produces a human-readable report you can share

No cloud keys needed. No paid APIs. Just **you + your laptop**.

---

## âœ¨ What it does

âœ… **Profiles** a dataset (types, nulls, ranges, cardinality)  
âœ… **Generates DQ rules** using an LLM (Ollama)  
âœ… **Validates rules** inside DuckDB (fast SQL checks)  
âœ… **Outputs a report** as Markdown (`outputs/report.md`)  
âœ… Built as a **LangGraph workflow** (clear, modular, extensible)

---

## ğŸ§  Why LangGraph?

Because data engineering pipelines arenâ€™t â€œone big scriptâ€.

They are **steps**, **state**, **guardrails**, and **retries**.

LangGraph makes that explicit:

`load â†’ profile â†’ propose_rules â†’ validate â†’ report`

Each step is a node.
The shared state moves through the graph.
Failures donâ€™t crash everything â€” they get captured as structured errors.

---

## ğŸ—ï¸ Architecture (High level)

- **DuckDB** = local analytics engine (no setup)
- **LangGraph** = orchestration / workflow graph
- **Ollama** = local LLM for rule generation
- **Markdown report** = simple output anyone can read

---

## ğŸš€ Quickstart

### 1) Start Ollama + pull a model
Install Ollama, then:

```bash
ollama pull qwen2.5:7b
