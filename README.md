# ğŸ™ LangGraph Data Quality Copilot

![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Local LLM](https://img.shields.io/badge/LLM-local--first-orange)

A **local-first, agentic data quality system** built with **LangGraph**, **DuckDB**, and **local LLMs (Ollama)**.

This project automates data profiling, rule generation, validation, and reporting â€” without cloud dependencies.

---

## ğŸ¯ What Problem This Solves

You receive CSV data from vendors, adâ€‘hoc exports, or internal teams.

You expect:
- missing values
- duplicates
- invalid ranges
- inconsistent formats

But writing and maintaining dozens of validation queries is slow and brittle.

This project replaces that manual work with an **agent-based data quality pipeline**.

---

## âœ¨ What It Does

âœ… Profiles datasets automatically  
âœ… Uses an LLM to propose quality rules  
âœ… Validates rules against real data using DuckDB  
âœ… Produces a humanâ€‘readable Markdown report  
âœ… Runs fully **offline** on your laptop

---

## ğŸ§  Why LangGraph (Not Traditional Scripts)

Traditional approach:
- Hardâ€‘coded SQL checks
- Manual updates per dataset
- No reasoning or explanation

LangGraph approach:
- Stateful workflow
- Clear node responsibilities
- Deterministic execution
- LLM reasoning only where needed

This makes the system **inspectable, extensible, and productionâ€‘shaped**.

---

## ğŸ— Architecture Overview

The system is implemented as a **LangGraph state machine**:

1. **Ingest Node** â€“ loads CSV into DuckDB
2. **Profile Node** â€“ computes nulls, ranges, distincts
3. **Rule Generator Node** â€“ LLM proposes validation rules
4. **Validation Node** â€“ rules executed via SQL
5. **Report Node** â€“ generates Markdown output

Each node updates a shared state object passed through the graph.

### Core Components

#### 1. Input CSV
- Raw dataset provided by a user or vendor  
- Example: `customer_data.csv`

#### 2. LangGraph Workflow
- Orchestrates each step as an explicit state
- Controls execution order and shared state
- Makes the pipeline deterministic and debuggable

#### 3. DuckDB
- Embedded analytical database
- Used for profiling and validation queries
- Fast, local, zero setup

#### 4. Local LLM (Ollama)
- Generates data quality rules from profiling statistics
- Runs fully offline
- No external API calls or credentials required

#### 5. Output Report
- Human-readable Markdown report
- Summarizes checks, failures, and observations  
- Example: `quality_report.md`

![Architecture Diagram](docs/architecture.png)

---

## ğŸ“‚ Project Structure

```
langgraph-data-quality-copilot/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ graph.py
â”‚   â”œâ”€â”€ state.py
â”‚   â”œâ”€â”€ nodes/
â”‚   â”‚   â”œâ”€â”€ ingest.py
â”‚   â”‚   â”œâ”€â”€ profile.py
â”‚   â”‚   â”œâ”€â”€ generate_rules.py
â”‚   â”‚   â”œâ”€â”€ validate.py
â”‚   â”‚   â””â”€â”€ report.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ data_quality_report.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Prerequisites

- Python **3.10+**
- Ollama installed and running

Pull a local model:
```bash
ollama pull llama3.1
```

---

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 3ï¸âƒ£ Run the Copilot

```bash
python -m src.main --input data/sample.csv
```

---

## ğŸ“„ Example Output

Console:
```
âœ” Dataset loaded into DuckDB
âœ” Profiling completed
âœ” 14 rules generated
âœ– 3 rule failures detected
âœ” Report saved to outputs/data_quality_report.md
```

Report excerpt:
```
Column: age
- Rule: age must be between 0 and 120
- Failures: 7 rows
- Suggested Fix: investigate negative values
```

---

## âš™ï¸ Configuration

Supported CLI options:

| Option | Description |
|------|------------|
| `--input` | Path to CSV file |
| `--model` | Ollama model name (default: llama3.1) |
| `--output` | Output report path |

---

## ğŸ§ª Testing

```bash
pytest tests/
```

---

## ğŸ”’ Privacy & Security

- No data leaves your machine
- No API keys required
- Fully offline execution

---

## ğŸ›  Extending the System

You can easily add:
- New validation strategies
- Different report formats (HTML / JSON)
- Cloud warehouses (Snowflake / BigQuery)
- CI validation on pull requests

---

## ğŸ“œ License

MIT License. See `LICENSE` file for details.

---

## ğŸ™Œ Acknowledgements

- LangGraph
- DuckDB
- Ollama

---

## â­ When This Is Useful

âœ” Vendor data validation  
âœ” Oneâ€‘off CSV audits  
âœ” Data engineering demos  
âœ” Learning agentic workflows  

---

If you want this adapted for **Snowflake**, **Databricks**, or **CI pipelines**, the architecture already supports it.

