# ğŸ™ LangGraph Data Quality Copilot  
**Local LLM + DuckDB + LangGraph**

An AI-assisted, **local-first data quality workflow** that profiles datasets, generates validation rules, runs checks, and produces a clear, human-readable report â€” all without cloud APIs.

---

## ğŸ“Œ What this project is

You receive a dataset â€” a vendor CSV, a client export, or a quick pull from S3.  
You already expect issues: missing values, duplicates, invalid ranges, or inconsistent types.

Instead of manually writing validation SQL every time, this project automates the workflow:

- Profile the dataset
- Let an AI suggest data quality rules
- Validate the data using those rules
- Generate a clear Markdown report

Everything runs **locally** using DuckDB and a local LLM (Ollama).

---

## ğŸ—ï¸ Architecture Overview

This project is built as a **state-driven workflow** using LangGraph.

### Core Components

#### 1. Input CSV
- Raw dataset provided by a user or vendor  
- Example: `customer_data.csv`

#### 2. LangGraph Workflow
- Orchestrates each step as an explicit state
- Controls execution order and shared state
- Makes the pipeline deterministic and debuggable

#### 3. DuckDB
- Embedded analytical database
- Used for profiling and validation queries
- Fast, local, zero setup

#### 4. Local LLM (Ollama)
- Generates data quality rules from profiling statistics
- Runs fully offline
- No external API calls or credentials required

#### 5. Output Report
- Human-readable Markdown report
- Summarizes checks, failures, and observations  
- Example: `quality_report.md`

![Architecture Diagram](docs/architecture.png)

---

## ğŸ”„ Workflow States (Conceptual)

Each box in the architecture diagram maps directly to a LangGraph state.

### 1. Load Data
- Reads the input CSV file
- Makes the dataset available to downstream states

### 2. Profile Data
- Computes dataset statistics such as:
  - Null counts
  - Distinct counts
  - Value ranges
  - Inferred data types

### 3. Generate Rules
- Sends profiling statistics to the local LLM
- Receives proposed data quality rules (e.g., null checks, range checks)

### 4. Validate Data
- Executes validation logic in DuckDB
- Evaluates data against generated rules

### 5. Generate Report
- Produces a Markdown report summarizing:
  - Applied rules
  - Failed checks
  - Observed data quality issues

---

## ğŸ” Workflow States (Mapped)

| State              | Responsibility                              |
|--------------------|---------------------------------------------|
| Load Data          | Reads CSV into memory                       |
| Profile Data       | Computes dataset statistics                 |
| Generate Rules     | AI proposes quality rules                   |
| Validate Data      | Runs validation using DuckDB                |
| Generate Report    | Writes a Markdown quality report            |

---

## ğŸ“¥ Input and ğŸ“¤ Output

### Input
- `data/customer_data.csv`

### Output
- `reports/quality_report.md`

---

## ğŸ“ Project Structure

```text
langgraph-data-quality-copilot/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_data.csv
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ graph.py
â”‚   â”œâ”€â”€ states/
â”‚   â”‚   â”œâ”€â”€ load_data.py
â”‚   â”‚   â”œâ”€â”€ profile_data.py
â”‚   â”‚   â”œâ”€â”€ generate_rules.py
â”‚   â”‚   â”œâ”€â”€ validate_data.py
â”‚   â”‚   â””â”€â”€ generate_report.py
â”‚   â”‚
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â””â”€â”€ duckdb_client.py
â”‚   â”‚
â”‚   â””â”€â”€ llm/
â”‚       â””â”€â”€ ollama_client.py
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ quality_report.md
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture.png
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## ğŸ§  Why LangGraph?

This workflow is not a single script. It is a sequence of steps with shared state.
LangGraph makes the pipeline explicit:
`load â†’ profile â†’ propose_rules â†’ validate â†’ report`
Each step is a node, state flows through the graph, and errors are captured instead of crashing the run.

---

## ğŸš€ Why this approach
- Rule discovery is automated
- Runs fully offline
- Easy to extend or replace components
- Useful for exploration, validation, and learning

---

## ğŸš« Non-goals
- Not a production data quality platform
- Not distributed or real-time
- Not multi-user

