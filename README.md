# ğŸ™ LangGraph Data Quality Copilot (Local LLM + DuckDB)

You get a CSV from somewhere â€” an email, a vendor export, a quick pull.
You expect missing values, duplicates, and odd ranges.

Instead of writing manual validation queries, this project lets the data describe its own quality rules.

This is a local-first data quality workflow that profiles a dataset, generates validation rules using a local LLM, runs checks in DuckDB, and produces a readable report.

No cloud setup. No paid APIs.

---

## âœ¨ What it does

âœ… **Profiles** a dataset (types, nulls, ranges, cardinality)  
âœ… **Generates DQ rules** using an LLM (Ollama)  
âœ… **Validates rules** inside DuckDB (fast SQL checks)  
âœ… **Outputs a report** as Markdown (`outputs/report.md`)  
âœ… Built as a **LangGraph workflow** (clear, modular, extensible)

---

## ğŸ§  Why LangGraph?

This workflow is not a single script. It is a sequence of steps with shared state.
LangGraph makes the pipeline explicit:
`load â†’ profile â†’ propose_rules â†’ validate â†’ report`
Each step is a node, state flows through the graph, and errors are captured instead of crashing the run.

---

## ğŸ—ï¸ Architecture (High level)

- **DuckDB** â€“ local analytics engine (no setup)
- **LangGraph** â€“ orchestration / workflow graph
- **Ollama** â€“ local LLM for rule generation
- **Markdown report** â€“ simple output anyone can read

---

## ğŸ“‚ Input & Output
**ğŸ“¥ Input**
examples/sample_data.csv

**ğŸ“¤ Output**
outputs/report.md

---

## ğŸš€ Why this approach
- Rule discovery is automated
- Runs fully offline
- Easy to extend or replace components
- Useful for exploration, validation, and learning

---

## ğŸš« Non-goals
- Not a production data quality platform
- Not distributed or real-time
- Not multi-user
